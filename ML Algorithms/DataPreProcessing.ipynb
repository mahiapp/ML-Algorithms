{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf79a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06629edb",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7865337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filename: str):\n",
    "    '''\n",
    "    Algorithm that reads in a csv file and returns \n",
    "    its contents as a Pandas dataframe\n",
    "    \n",
    "    Performs tasks specific to individual datasets\n",
    "    \n",
    "    @param filename: name of the csv file to be converted to dataframe\n",
    "    '''\n",
    "    \n",
    "    #Sets default of header names as empty\n",
    "    header_columns = None\n",
    "    \n",
    "    #Assigns column headers for each dataset\n",
    "    if 'breast-cancer-wisconsin' in filename:\n",
    "        header_columns = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size',\n",
    "        'Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei',\n",
    "        'Bland Chromatin','Normal Nucleoli','Mitoses','Class']\n",
    "        \n",
    "    elif 'car' in filename:\n",
    "        header_columns = ['buying','maint','doors',\n",
    "        'persons','lug_boot','safety','acceptability']\n",
    "        \n",
    "    elif 'house-votes-84' in filename:\n",
    "        header_columns = ['Class Name','handicapped-infants','water-project-cost-sharing', 'adoption-of-the-budget-resolution',\n",
    "        'physician-fee-freeze','el-salvador-aid','religious-groups-in-schools','anti-satellite-test-ban',\n",
    "        'aid-to-nicaraguan-contras','mx-missile','immigration','synfuels-corporation-cutback','education-spending',\n",
    "        'superfund-right-to-sue','crime','duty-free-exports','export-administration-act-south-africa']\n",
    "        \n",
    "    elif 'abalone' in filename:\n",
    "        header_columns = ['Sex','Length', 'Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight',\n",
    "        'Rings']\n",
    "        \n",
    "    elif 'machine' in filename:\n",
    "        header_columns = ['vendor name','Model', 'MYCT', 'MMIN','MMAX','CACH','CHMIN','CHMAX','PRP','ERP']\n",
    "        \n",
    "    \n",
    "    #Reads csv file and converts to dataframe\n",
    "    file_data = pd.read_csv(filename, index_col = False, names = header_columns)     \n",
    "    \n",
    "    #Drops non-feature columns from dataframe\n",
    "    if 'breast-cancer-wisconsin' in filename: \n",
    "        file_data = file_data.drop(columns=['Sample code number'])\n",
    "    \n",
    "    if 'machine' in filename: \n",
    "        file_data = file_data.drop(columns=['vendor name','Model'])\n",
    "    \n",
    "    \n",
    "    return file_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4385e",
   "metadata": {},
   "source": [
    "# Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db6c035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImputeMissingValues(dataset, false_missing_values: bool, missing_attribute_value = '?', missing_attribute_replacement = None, contains_missing_values = False):\n",
    "    '''\n",
    "    Replaces missing values in dataset\n",
    "    \n",
    "    @param dataset: dataframe to handle missing data for\n",
    "    @param false_missing_values: a True or False boolean value that, if true, inidicates that the missing attribute \n",
    "                                    does not represent missing data, and if false, represents missing data\n",
    "    @param missing_attribute_value: the representation of the missing value in the dataset. Default set to '?'\n",
    "    @param missing_attribute_replacement: Provides the value to replace with when the missing attribute does not indicate \n",
    "                                        missing data(only needed when false_missing_values is True)\n",
    "    @param contains_missing_values: a True or False boolean value that, when True, indicates that the dataset has\n",
    "                                    missing values, and when False, contains no missing values\n",
    "    '''\n",
    "    #If dataset does not contain missing values, function ends here\n",
    "    if contains_missing_values != True:\n",
    "        return dataset\n",
    "    \n",
    "    #If dataset contains missing values, the false_missing_values argument is checked to determine what the missing value represents\n",
    "    else:\n",
    "        #When data is actually missing, the mean of the feature is used to replace the missing values\n",
    "        if false_missing_values != True:\n",
    "            #Each column is checked for specified missing attribute\n",
    "            for column in dataset:  \n",
    "                #If missing values exists in the column, the column mean is calculated and replaces the missing values\n",
    "                if missing_attribute_value in dataset[column].values:\n",
    "                    feature_mean = int(pd.to_numeric(dataset[column],errors ='coerce').mean())\n",
    "                    dataset[column] = dataset[column].replace(missing_attribute_value, feature_mean)\n",
    "        #When data is not actually missing, the missing attribute is replaced with the specified replacement value\n",
    "        else:\n",
    "            for column in dataset:\n",
    "                if missing_attribute_value in dataset[column].values:\n",
    "                    dataset[column] = dataset[column].replace(missing_attribute_value, missing_attribute_replacement)\n",
    "        return dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2241f82",
   "metadata": {},
   "source": [
    "# Handling Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efabc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncodeOrdinalData(dataset, ordinal_mapping = None):\n",
    "    '''\n",
    "    Encodes ordinal data using integer mapping \n",
    "    \n",
    "    @param dataset: dataframe to be encoded\n",
    "    @param ordinal_mapping: a nested dictionary mapping each value in columns with ordinal data to integers\n",
    "                            (When empty, no nominal data is assumed.)\n",
    "    '''\n",
    "    \n",
    "    #If dataset contains ordinal data, use dictionary values to encode data\n",
    "    if ordinal_mapping != None:\n",
    "        for attribute in ordinal_mapping:\n",
    "            #For each column with ordinal data, replace values with encoding integers specified in mapping dictionary\n",
    "            for key in ordinal_mapping[attribute]:\n",
    "                dataset[attribute] = dataset[attribute].replace(key, ordinal_mapping[attribute][key])\n",
    "        return dataset\n",
    "    return dataset\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a56103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncodeNominalData(dataset, nominal_column_names=None):\n",
    "    '''\n",
    "    Encodes nominal data using one-hot encoding\n",
    "    \n",
    "    @param dataset: dataframe to be encoded\n",
    "    @param nominal_column_names: a list of column names containing nominal data. When empty, \n",
    "                                 no nominal data is assumed.\n",
    "    '''\n",
    "    if nominal_column_names != None:\n",
    "        #Loops through each specfied nominal column, gets the unique values from those columns, \n",
    "        #and stores it in a list 'category_values'\n",
    "        for column in nominal_column_names:\n",
    "            category_values = dataset[column].unique()\n",
    "            num_categories = len(category_values)\n",
    "            one_hot_encoded_data = [0]*num_categories\n",
    "            #An empty list of 0s the size of the number of unique values is created, and in each iteration,\n",
    "            #the corresponding binary value is generated in list form\n",
    "            for number in range(num_categories):\n",
    "                empty_dummies = [0]*num_categories\n",
    "                empty_dummies[number]=1\n",
    "                one_hot_encoded_data[number]=str(empty_dummies)\n",
    "            \n",
    "            #Sets all values in the new binary columns 0\n",
    "            dataset[category_values] = 0\n",
    "            \n",
    "            #Changes the binary column values to a corresponding 1 if the column name corresponds with\n",
    "            #the value at that index in the original nominal column\n",
    "            for value in category_values:\n",
    "                dataset[value].mask(dataset[column] == value, 1, inplace=True)\n",
    "        #Drops the original nominal column\n",
    "        dataset = dataset.drop(columns=nominal_column_names)\n",
    "                                  \n",
    "        return dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa37404",
   "metadata": {},
   "source": [
    "# Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b00980d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discretize(dataset, num_bins, discretization_type):\n",
    "    '''\n",
    "    Discretizes the data into groups and replaces the original data of these groups with a single value\n",
    "    \n",
    "    @param dataset: dataframe to be discretized\n",
    "    @param num_bins: number of groups to separate the data into\n",
    "    @param: discretization_type: either \"frequency\" or \"width\". Indicates whether to perform equal-width \n",
    "                                discretization or equal-frequency discretization\n",
    "    '''\n",
    "    #Initialing the dataframe which will be used to store and return the final results\n",
    "    discretized_df = pd.DataFrame()\n",
    "    \n",
    "    #Loops through each column to perform discretization on each feature\n",
    "    for column in dataset:\n",
    "        #Calculates the size of each bin by using range of data divided by number of bins\n",
    "        max_val = dataset[column].max()\n",
    "        min_val = dataset[column].min()\n",
    "        bin_size = (max_val - min_val)/num_bins\n",
    "        \n",
    "        #Equal-frequency discretization\n",
    "        if discretization_type == 'width':\n",
    "            #Initializes the dictionary which will map each bin to its range/size\n",
    "            bin_ranges = {}\n",
    "            count = 0\n",
    "            \n",
    "            #Loops through each bin, and calculates the range for that bin, adds all the values \n",
    "            #in the column within that range to the bin, and adds the values in the bin to the bin_ranges dict\n",
    "            for index in range(num_bins):\n",
    "                bin_values = []\n",
    "                current_range = [int(count),int(count+bin_size)]\n",
    "                bin_ranges[str(current_range)]={}\n",
    "                for value in dataset[column]:\n",
    "                    if value in range(current_range[0],current_range[1]):\n",
    "                        bin_values.append(value)\n",
    "                bin_ranges[str(current_range)] = bin_values\n",
    "                count = current_range[1]\n",
    "            return bin_ranges\n",
    "        \n",
    "        #Equal-frequency discretization\n",
    "        elif discretization_type == 'frequency':\n",
    "            #Splits the dataevenly into the as many parts as the specified number of bins\n",
    "            bins = np.array_split((dataset[column].to_numpy()),num_bins)\n",
    "            new_feat_list = []\n",
    "            #Calculates the average of each bin, and adds the values to a list\n",
    "            for i in bins:\n",
    "                new_feat_list.append(i.mean())\n",
    "                \n",
    "        #Adds the list averages to the final dataframe to be returned\n",
    "        discretized_df[column] = new_feat_list\n",
    "    return discretized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04e451",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62526475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardize(train_data,test_data):\n",
    "    '''\n",
    "    Standardize training data using z-score standardization and apply to test data\n",
    "    \n",
    "    @param train_data: training data\n",
    "    @param test_data: test data\n",
    "    \n",
    "    '''\n",
    "    for feature in train_data:\n",
    "        #Calculate the average and standard deviation of the training data\n",
    "        avg = train_data[feature].mean()\n",
    "        std = train_data[feature].std()\n",
    "        z_score_train_list = []\n",
    "        z_score_test_list = []\n",
    "        \n",
    "        #Calculate z_score for each point in the train data and add to list\n",
    "        for x in train_data[feature]:\n",
    "            z_score = (x-avg)/std\n",
    "            z_score_train_list.append(z_score)\n",
    "            \n",
    "        #Calculate z_score for each point in the test data  \n",
    "        for x in test_data[feature]:\n",
    "            z_score = (x-avg)/std\n",
    "            z_score_test_list.append(z_score)\n",
    "            \n",
    "        #Change each column to standardized data\n",
    "        train_data[feature] = z_score_train_list\n",
    "        test_data[feature] = z_score_test_list\n",
    "        \n",
    "    return train_data,test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
