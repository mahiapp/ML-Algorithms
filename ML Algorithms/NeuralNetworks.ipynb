{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf79a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations,permutations\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "%run DataPreProcessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc6f5f",
   "metadata": {},
   "source": [
    "# Activation and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26c9d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(inp):\n",
    "    '''\n",
    "    Sigmoid function calculates the sigmoid by the taking the exponent of the negative input, \n",
    "    adding it to 1, and dividing 1 by the previous sum\n",
    "    \n",
    "    @param inp: input value to calculate sigmoid of(can be indiviudal number or array)  \n",
    "    '''\n",
    "    return 1/(1+np.exp(-inp))\n",
    "\n",
    "def Loss(true,pred,task_type):\n",
    "    '''\n",
    "    Calculates loss of the gradient descent\n",
    "    \n",
    "    @param true: the target values of the function\n",
    "    @param pred: the predicted values of the function\n",
    "    @param task_type: Indicates if the dataset is a classification or regression set\n",
    "    '''\n",
    "    #Calculates and returns cross entropy loss for classificaction tasks\n",
    "    if task_type == 'Classification':\n",
    "        cross_entropy_loss = -np.sum(true * np.log(pred))\n",
    "        return cross_entropy_loss\n",
    "    \n",
    "    #Calculates and returns mean squared error for regression tasks\n",
    "    if task_type == 'Regression':\n",
    "        MSE = np.square(np.subtract(true,pred)).mean()\n",
    "        return MSE\n",
    "    \n",
    "    \n",
    "def Accuracy(pred,true):\n",
    "    accuracy = pred.argmax(axis=1) == true.argmax(axis=1)\n",
    "    return accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8cbf9",
   "metadata": {},
   "source": [
    "# Linear Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "869c9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(dataset, class_label,train=False,test=False,trained_weights=None):\n",
    "    '''\n",
    "    Logistic regression algorithm predicts the outcomes of a linear classification network by initializing\n",
    "    weights to each feature and calculating the softmax of each input to get the prediction. Updates weights \n",
    "    and repeats until convergence\n",
    "    \n",
    "    @param dataset: dataframe\n",
    "    @param class_label: name of class column\n",
    "    @param train: true if model is being trained\n",
    "    @param test: true if model is being tested\n",
    "    @param trained_weights: the weights array from trained model(only needed when test == True)    \n",
    "    '''\n",
    "    \n",
    "    #Initilize the target columns and features columns, as well as the inidividual classes\n",
    "    target = dataset[class_label].to_numpy()\n",
    "    features = dataset.drop(columns = [class_label]).to_numpy()\n",
    "    classes = list(dataset[class_label].unique())\n",
    "    num_classes = len(classes)\n",
    "    num_features = len(features[0])\n",
    "    \n",
    "    #Initialize the learning rate to be used in the weight update as well as the number of iterations until convergence\n",
    "    step_size = (1/len(target))*0.01\n",
    "    iterations = 100\n",
    "    \n",
    "    #Add biases column to features\n",
    "    biases = np.ones((len(target),1))\n",
    "    features = np.hstack((biases,features))\n",
    "    \n",
    "    #Initialize the dataframe to keep track of the loss over the iterations(to be used in graph)\n",
    "    loss_df = pd.DataFrame(columns=['Loss'])\n",
    "       \n",
    "    #Perform one hot encoding of class label column in order to be used for the target predicted comparison\n",
    "    target_encoded=np.zeros((len(target),num_classes))      \n",
    "    for i in range(0,len(target)):\n",
    "        class_index = classes.index(target[i])\n",
    "        target_encoded[i][class_index]=1\n",
    "\n",
    "    #Inititalize weights randomly between -0.01 and 0.01\n",
    "    if train == True:       \n",
    "        weights = np.zeros((num_classes,num_features+1))\n",
    "        for i in range (0,num_classes):\n",
    "            for j in range(0,num_features):\n",
    "                weights[i][j]=random.uniform(-0.01,0.01)\n",
    "\n",
    "    new_weights = np.zeros((num_classes,num_features+1))\n",
    "    delta_weights = np.zeros((num_classes,num_features+1))\n",
    "     \n",
    "    #Use trained weights as weights during testing   \n",
    "    if test == True:\n",
    "        weights=trained_weights\n",
    "        \n",
    "    #Begin loop to make predictions and update weights\n",
    "    k=0\n",
    "    while k<iterations:\n",
    "        final_predictions = []\n",
    "        row_index=0\n",
    "        \n",
    "        #Initialize the change in weights to 0 during each iteration\n",
    "        for i in range(0,num_classes):\n",
    "            for j in range(0,num_features+1):\n",
    "                delta_weights[i][j]=0 \n",
    "        #Loop through each row in features \n",
    "        for row in features:\n",
    "            outputs = [0]*num_classes\n",
    "            predictions = [0]*num_classes\n",
    "            \n",
    "            print('')\n",
    "            print('Weight*Input Calculation') \n",
    "            #Calculate the output value for each input row by multipliying the weights with the feature values\n",
    "            for i in range(0,num_classes):\n",
    "                for j in range(0,num_features+1):\n",
    "                    outputs[i]=outputs[i]+weights[i][j]*row[j]  \n",
    "            print(outputs)\n",
    "            print('Softmax Activation Calculation')\n",
    "            #Calculate softmax of the outputs, which is the prediction of that row       \n",
    "            exp_sum = np.sum(np.exp(outputs))\n",
    "            for i in range(0,num_classes):\n",
    "                predictions[i]= (np.exp(outputs[i]))/exp_sum\n",
    "            print(predictions)\n",
    "            #Add the predictions to a list to keep track of the predictions for each input vector\n",
    "            final_predictions.append(predictions)\n",
    "            \n",
    "            #Calculate the change in weights by calculating the differnece in the target and predicted values and muliplying by feature values\n",
    "            for i in range(0,num_classes):\n",
    "                target_val = target_encoded[row_index][i]\n",
    "                predicted_val = predictions[i]\n",
    "                for j in range(0,num_features+1):\n",
    "                    delta_weights[i][j]=delta_weights[i][j]+(target_val-predicted_val)*row[j]\n",
    "        \n",
    "            row_index = row_index+1 \n",
    "        #When testing, return the predicted and final arrays as the output after 1 iteration\n",
    "        if test == True:\n",
    "            return [np.asarray(final_predictions),target_encoded]\n",
    "        \n",
    "        loss_df=loss_df.append({\"Loss\":Loss(target_encoded,final_predictions,'Classification')},ignore_index=True)\n",
    "        print('Weights Before Update')\n",
    "        print(weights)\n",
    "        #Update the weights by adding the change in weights*the learning rate to each of the existing weights\n",
    "        print('Gradient Calculation')\n",
    "        for i in range(0,num_classes):\n",
    "            for j in range(0,num_features+1):\n",
    "                print(delta_weights[i][j])\n",
    "                weights[i][j]=weights[i][j]+delta_weights[i][j]*step_size\n",
    "        print('Weights After Update')\n",
    "        print(weights)\n",
    "        print('')\n",
    "        k=k+1\n",
    "    #Plot the loss and return the final weights  \n",
    "    loss_df.Loss.plot(title='Loss')\n",
    "    return weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae23f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegressionNetwork(dataset,class_label,train=False,test=False,trained_weights=None):\n",
    "    '''\n",
    "    Linear regression algorithm predicts the outcomes of a linear classification network by initializing\n",
    "    weights to each feature and calculating the dot products of the weights and features to get the prediction. \n",
    "    Updates weights and repeats until convergence\n",
    "    \n",
    "    @param dataset: dataframe\n",
    "    @param class_label: name of class column\n",
    "    @param train: true if model is being trained\n",
    "    @param test: true if model is being tested\n",
    "    @param trained_weights: the weights array from trained model(only needed when test == True) \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Initilize the target columns and features columns, as well as the inidividual classes\n",
    "    target = dataset[class_label].to_numpy()\n",
    "    sample_size = len(target)\n",
    "    features = dataset.drop(columns = [class_label]).to_numpy()\n",
    "    num_features = len(features[0])\n",
    "    \n",
    "    #Initialize the learning rate to be used in the weight update as well as the number of iterations until convergence\n",
    "    learning_rate = 0.01\n",
    "    iterations = 100\n",
    "    \n",
    "    #Add intercepts column to features\n",
    "    beta_0 = np.ones((sample_size,1)) \n",
    "    features = np.hstack((beta_0,features))\n",
    "    \n",
    "    #Initialize the dataframe to keep track of the loss over the iterations(to be used in graph)\n",
    "    loss_df = pd.DataFrame(columns=['Loss'])\n",
    "    \n",
    "    #Initialize the weights of each feature randomly between -0.01 and 0.01\n",
    "    k=0\n",
    "    if train == True:       \n",
    "        weights = np.zeros(num_features+1)\n",
    "        for j in range(0,num_features):\n",
    "            weights[j]=random.uniform(-0.01,0.01)\n",
    "        \n",
    "    #Use trained weights as weights during testing \n",
    "    if test == True:\n",
    "        weights=trained_weights\n",
    "    \n",
    "    #Begin loop to make predictions and update weights\n",
    "    while k<iterations:\n",
    "        \n",
    "        #Initialize the change in weights to 0 during each iteration\n",
    "        delta_weights = np.zeros_like(weights)\n",
    "        \n",
    "        #Caclulate the prediction by taking the dot product of the features and weights\n",
    "        print('')\n",
    "        print('Weight*Input Calculation') \n",
    "        pred = features.dot(weights)\n",
    "        print(pred)\n",
    "        print('')\n",
    "        target = target.reshape(pred.shape)\n",
    "        \n",
    "        #When testing, return the predicted and final arrays as the output after 1 iteration\n",
    "        if test == True:\n",
    "            return [pred,target]\n",
    "        \n",
    "        loss_df=loss_df.append({\"Loss\":Loss(target,pred,'Regression')},ignore_index=True)\n",
    "        \n",
    "        #Calculate change in weights by taking the differences of the target and predictions and mulyiplying \n",
    "        #by the features and dividing by sample size\n",
    "        delta_weights = (((target-pred).T).dot(features))/sample_size\n",
    "        \n",
    "        print('Weights Before Update')\n",
    "        print(weights)\n",
    "        #Update the weights by adding the change in weights*the learning rate to each of the existing weights\n",
    "        weights = weights + delta_weights*learning_rate\n",
    "        print('Weights After Update')\n",
    "        print(weights)\n",
    "        k=k+1\n",
    "    #Plot the loss and return the final weights  \n",
    "    loss_df.Loss.plot(title='Loss')\n",
    "    return weights\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef3bdd",
   "metadata": {},
   "source": [
    "# Hidden Layer Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29270c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHiddenCombos(num_hidden_layers,num_features):\n",
    "    '''\n",
    "    Gets all possible combos of hidden layer node sizes \n",
    "    \n",
    "    @param num_hidden_layers: the number of hidden layers in the network\n",
    "    @param: num_features: number of features the input has    \n",
    "    '''\n",
    "    #gets the possible node size values from 1 to 1 less than tbe number o features since the hidden layer\n",
    "    #nodes must be smaller than the input layer\n",
    "    features_range = list(range(1,num_features))\n",
    "    \n",
    "    #gets all possible combos of node sizes by calculating the permutations of the features range list. Each combo\n",
    "    #is of the size of the number of layers, so that one node size value correlates to each layer\n",
    "    combos = list(permutations(features_range,num_hidden_layers))\n",
    "    \n",
    "    #The permutations function does not include combos with the same values, such as (1,1). Therefore,\n",
    "    #this for loop adds all matching node size combos to the permutations list\n",
    "    for i in range(1,num_features):\n",
    "        combos.append(tuple([i,i]))\n",
    "        \n",
    "    return combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03f74af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TuneHidden(model_type,task_type, dataset, class_label, num_features):\n",
    "    '''\n",
    "    Tunes the best hidden layer nodes combo by running through all the combos from the GetHiddenCombos function,\n",
    "    and testing them on the needed function to determine which has the best performance based on the lowest error\n",
    "    \n",
    "    @param model_type: indicates if this is a 2 hidden layer feedforward neural network, or a an autoencoder network\n",
    "    @param task_type: indicates the dataset as a classification or regression dataset\n",
    "    @param dataset: input dataframe\n",
    "    @param class_label: name of class column\n",
    "    @param num_features: number of features in the dataset\n",
    "    '''\n",
    "    \n",
    "    #gets all possible node size combos for 2 hidden layer networks based on the \n",
    "    #number of features using the GetHiddenCombos function\n",
    "    combos = GetHiddenCombos(2,num_features)\n",
    "    \n",
    "    #initializes a dictionary to track performance of each combo \n",
    "    performance = {}\n",
    "    \n",
    "    \n",
    "    #tunes hidden layers for feedforward neural networks\n",
    "    if model_type == 'FF':\n",
    "        \n",
    "        #iterates through all the combos and for each combo, trains the feedforward network, tests it,\n",
    "        #and stores the error in a dictionary\n",
    "        for combo in combos:\n",
    "            weights = FeedForwardNN_2H(dataset,class_label,list(combo),task_type,train=True,tuning=True)\n",
    "            model = FeedForwardNN_2H(dataset,class_label,list(combo),task_type,test=True,trained_weights=weights,tuning=True)\n",
    "            error = Evaluate(task_type,model[0],model[1])\n",
    "            performance[combo]= error\n",
    "            \n",
    "        #gets the best performing combo by getting the key of the lowest error value in the dict\n",
    "        best_combo = min(performance, key=performance.get)\n",
    "\n",
    "    #tunes hidden layers for auotoencoder neural networks\n",
    "    if model_type == 'Autoencoder':\n",
    "        \n",
    "        #iterates through all the combos and for each combo, trains the autoencoder network, tests it,\n",
    "        #and stores the error in a dictionary\n",
    "        for combo in combos:\n",
    "            weights = Autoencoder(dataset,class_label,list(combo),task_type,train=True,tuning=True)\n",
    "            model = Autoencoder(dataset,class_label,list(combo),task_type,test=True,trained_weights=weights,tuning=True)\n",
    "            error = Evaluate(task_type,model[0],model[1])\n",
    "            performance[combo]= error\n",
    "            \n",
    "        #gets the best performing combo by getting the key of the lowest error value in the dict\n",
    "        best_combo = min(performance, key=performance.get)\n",
    "    \n",
    "    #returns the best combo to use\n",
    "    return list(best_combo)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331666e",
   "metadata": {},
   "source": [
    "\n",
    "# Feedforward NN with 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d534b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForwardNN_2H(dataset,class_label,nodes_sizes:list,task_type,train=False,test=False,trained_weights=None,tuning=False):\n",
    "    '''\n",
    "    Feed forward neural network with 2 hidden layers that moves forward through the differeny layers using the weights at each\n",
    "    layer, then backpropagates to update the weights. Process is repeated until convergence\n",
    "    \n",
    "    @param dataset: dataframe\n",
    "    @param class_label: name of class column\n",
    "    nodes_sizes: a size 2 list that indicates the number of nodes of each of the hidden layers\n",
    "    @param task_type: indicates whether dataset is a classification or regression dataset\n",
    "    @param train: true if model is being trained\n",
    "    @param test: true if model is being tested\n",
    "    @param trained_weights: the weights array from trained model(only needed when test == True)\n",
    "    @param tuning: if True, indicates that tuning of the hidden layers is being performed\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Initilize the target columns and features columns, as well as the individual classes\n",
    "    target = dataset[class_label].to_numpy()\n",
    "    features = dataset.drop(columns = [class_label]).to_numpy()\n",
    "    classes = list(dataset[class_label].unique())\n",
    "    num_classes=len(classes)\n",
    "    num_features = len(features[0])\n",
    "    \n",
    "    #Initialize the dataframe to keep track of the loss over the iterations(to be used in graph)\n",
    "    loss_df = pd.DataFrame(columns=['Loss'])\n",
    "       \n",
    "    #Initialize the learning rate to be used in the weight update as well as the number of iterations until convergence\n",
    "    learning_rate = 0.1\n",
    "    iterations = 100\n",
    "    sample_size = target.size\n",
    "    \n",
    "    \n",
    "    #Performs one hot encoding of class column for classification tasks in order to be used for the target predicted comparison\n",
    "    if task_type == 'Classification':\n",
    "        output_size=len(classes) \n",
    "        target_encoded=np.zeros((len(target),output_size))  \n",
    "        for i in range(0,len(target)):\n",
    "            class_index = classes.index(target[i])\n",
    "            target_encoded[i][class_index]=1\n",
    "        target=target_encoded\n",
    "        \n",
    "    #Sets the output size to 1 for regression tasks\n",
    "    if task_type == 'Regression':\n",
    "        output_size = 1\n",
    "    \n",
    "    #Initializes weights for the input to 1st hidden layer, 1st hidden layer to 2nd hidden layer,\n",
    "    #and the second hidden layer to the output layer\n",
    "    if train==True:\n",
    "        weights_inp_h1= np.random.uniform(-0.01,0.01,(num_features, nodes_sizes[0]))\n",
    "        weights_h1_h2= np.random.uniform(-0.01,0.01,(nodes_sizes[0],nodes_sizes[1]))\n",
    "        weights_h2_out= np.random.uniform(-0.01,0.01,(nodes_sizes[1],output_size))\n",
    "        \n",
    "    ##Use trained weights as weights during testing \n",
    "    if test==True:\n",
    "        weights_inp_h1= trained_weights[0]\n",
    "        weights_h1_h2= trained_weights[1]\n",
    "        weights_h2_out= trained_weights[2]\n",
    "    \n",
    "    #Begin loop to make predictions and update weights\n",
    "    k=0\n",
    "    while k<iterations:    \n",
    "\n",
    "        #FEEDFORWARD\n",
    "        \n",
    "        #Implementing feedforward propagation to calcuate 1st hidden layer using the logistic activation function\n",
    "        print('Hidden Layer 1 Calculation: sigmoid(weights*features)')\n",
    "        z_h1 = np.dot(features, weights_inp_h1)\n",
    "        hidden_1 = sigmoid(z_h1)\n",
    "        print(hidden_1)\n",
    "        \n",
    "        #Implementing feedforward propagation to get 2nd hidden layer\n",
    "        print('Hidden Layer 2 Calculation: sigmoid(weights*hidden layer 1)')\n",
    "        z_h2 = np.dot(hidden_1, weights_h1_h2)\n",
    "        hidden_2 = sigmoid(z_h2)\n",
    "        ##print(hidden_2)\n",
    "        \n",
    "        #Implementing feedforward propagation to get output layer\n",
    "        print('Output Layer Calculation: sigmoid(weights*hidden layer 2)')\n",
    "        z_output = np.dot(hidden_2, weights_h2_out)\n",
    "        prediction = sigmoid(z_output)      \n",
    "        print(prediction)\n",
    "        target=target.reshape(prediction.shape)\n",
    "         \n",
    "        \n",
    "        #When testing, return the predicted and final arrays as the output after 1 iteration\n",
    "        if test == True:\n",
    "            return [prediction,target]\n",
    "        \n",
    "        loss_df=loss_df.append({\"Loss\":Loss(target,prediction,task_type)},ignore_index=True)\n",
    "        \n",
    "        #BACKPROPAGATION\n",
    "        print('')\n",
    "        print('Backpropagation')\n",
    "        #weights change output to hidden 2(dW2)\n",
    "        weights_out_h2 = (prediction - target) * prediction * (1 - prediction)\n",
    "        #weights change hidden 2 to hidden 1\n",
    "        weights_h2_h1 = (np.dot(weights_out_h2, weights_h2_out.T)) * hidden_2 * (1 - hidden_2)\n",
    "        #weights change hidden 1 to input\n",
    "        weights_h1_inp = (np.dot(weights_h2_h1, weights_h1_h2.T)) * hidden_1 * (1 - hidden_1)\n",
    "\n",
    "\n",
    "        # Updating the weights\n",
    "        \n",
    "        #hidden 2 to outer update = hidden 2*weights change output to hidden 2\n",
    "        delta_weights_h2_out = np.dot(hidden_2.T,weights_out_h2)/sample_size\n",
    "        \n",
    "        #hidden 1 to hidden 2 update = hidden 1*weights change hidden 2 to hidden 1\n",
    "        delta_weights_h1_h2 = np.dot(hidden_1.T, weights_h2_h1) / sample_size\n",
    "        \n",
    "        #input to hidden 1 update = input features*weights change hidden 1 to input\n",
    "        delta_weights_inp_h1 = np.dot(features.T, weights_h1_inp) / sample_size\n",
    "\n",
    "      #  '''\n",
    "        print('Weights Before Update')\n",
    "        print('Input to Hidden 1')\n",
    "        print(weights_inp_h1)\n",
    "        print('Hidden 1 to Hidden 2')\n",
    "        print(weights_h1_h2)\n",
    "        print('Hidden 2 to Output')\n",
    "        print(weights_h2_out)\n",
    "       # '''\n",
    "        #Update weights by subtracting change in weights from current weight values\n",
    "        weights_h2_out = weights_h2_out - learning_rate * delta_weights_h2_out\n",
    "        weights_h1_h2 = weights_h1_h2 - learning_rate * delta_weights_h1_h2\n",
    "        weights_inp_h1 = weights_inp_h1 - learning_rate * delta_weights_inp_h1\n",
    "        #'''\n",
    "        print('')\n",
    "        print('Weights After Update')\n",
    "        print('Input to Hidden 1')\n",
    "        print(weights_inp_h1)\n",
    "        print('Hidden 1 to Hidden 2')\n",
    "        print(weights_h1_h2)\n",
    "        print('Hidden 2 to Output')\n",
    "        print(weights_h2_out)\n",
    "        print('')\n",
    "        print('------------------------')\n",
    "        print('')\n",
    "       # '''\n",
    "        k=k+1\n",
    "    \n",
    "    #Doesn't produce loss graph when tuning hidden layers\n",
    "    if tuning == False:  \n",
    "        loss_df.Loss.plot(title='Loss')\n",
    "        \n",
    "    #returns the final trained weight values\n",
    "    return [weights_inp_h1,weights_h1_h2,weights_h2_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7aada",
   "metadata": {},
   "source": [
    "# NN with Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aba69f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Autoencoder(dataset,class_label,nodes_sizes,task_type,train=False,test=False,trained_weights=None,tuning=False):\n",
    "    '''\n",
    "    Autoencoder network first trains a 1 hidden layer feedforward network, and the output equivalent to the input vector size.\n",
    "    Once trained, the output layer is dropped and the hidden layer is stored as the autoencoder. Then, a 2 hidden layer feedforward network\n",
    "    is implemented with the trained autoencoder being the first hidden layer, a new second hidden layer, and an output classification/\n",
    "    regression layer\n",
    "    \n",
    "    @param dataset: dataframe\n",
    "    @param class_label: name of class column\n",
    "    nodes_sizes: a size 2 list that indicates the number of nodes of each of the hidden layers\n",
    "    @param task_type: indicates whether dataset is a classification or regression dataset\n",
    "    @param train: true if model is being trained\n",
    "    @param test: true if model is being tested\n",
    "    @param trained_weights: the weights array from trained model(only needed when test == True)\n",
    "    @param tuning: if True, indicates that tuning of the hidden layers is being performed\n",
    "    \n",
    "    '''\n",
    "    #Initilize the target columns and features columns, as well as the individual classes\n",
    "    target = dataset[class_label].to_numpy()\n",
    "    features = dataset.drop(columns = [class_label]).to_numpy()\n",
    "    classes = list(dataset[class_label].unique())\n",
    "    num_classes=len(classes)\n",
    "    num_features = len(features[0])\n",
    "    \n",
    "    #Initialize the learning rate to be used in the weight update as well as the number of iterations until convergence\n",
    "    learning_rate = 0.1\n",
    "    iterations = 100\n",
    "    sample_size = target.size\n",
    "\n",
    "    #Initialize the dataframe to keep track of the loss over the iterations when training the encoder(to be used in graph)\n",
    "    encoder_loss_df = pd.DataFrame(columns=['Loss'])\n",
    "    \n",
    "    \n",
    "    #Initializes weights for the input to hidden layer, and the hidden layer to the output layer\n",
    "    if train == True:\n",
    "        weights_inp_encoder= np.random.uniform(-0.01,0.01,(num_features, nodes_sizes[0]))\n",
    "        weights_encoder_out= np.random.uniform(-0.01,0.01,(nodes_sizes[0],num_features))\n",
    "    \n",
    "    #Use trained weights as weights during testing \n",
    "    if test==True:\n",
    "        weights_inp_encoder= trained_weights[0]\n",
    "        weights_encoder_out = trained_weights[1]\n",
    "\n",
    "     #Begin loop to make predictions and update weights\n",
    "    k=0\n",
    "    ##print('Encoder Training')\n",
    "    while k<iterations:    \n",
    "\n",
    "        #FEEDFORWARD\n",
    "        #Same process as two hidden layer feedforward network except with just one hidden layer\n",
    "        print('Encoder Layer Calculation: sigmoid(weights*features)')\n",
    "        z_encoder = np.dot(features, weights_inp_encoder)\n",
    "        encoder = sigmoid(z_encoder)\n",
    "        print(encoder)\n",
    "        \n",
    "        ##print('Output Layer Calculation: sigmoid(weights*encoder)')\n",
    "        z_output = np.dot(encoder, weights_encoder_out)\n",
    "        encoder_output = sigmoid(z_output)\n",
    "        print(encoder_output)\n",
    "        \n",
    "        encoder_loss_df=encoder_loss_df.append({\"Loss\":Loss(encoder_output,features,task_type)},ignore_index=True)\n",
    "        \n",
    "        print('')\n",
    "        print('Backpropagation')\n",
    "        #BACKPROPAGATION\n",
    "        weights_out_encoder = (encoder_output - features) * encoder_output * (1 - encoder_output)\n",
    "        weights_encoder_inp = (np.dot(weights_out_encoder, weights_encoder_out.T)) * encoder * (1 - encoder)\n",
    "\n",
    "\n",
    "        # Updating the weights\n",
    "        delta_weights_encoder_out = np.dot(encoder.T,weights_out_encoder)/sample_size\n",
    "        delta_weights_inp_encoder = np.dot(features.T, weights_encoder_inp) / sample_size\n",
    "\n",
    "        #'''\n",
    "        print('Weights Before Update')\n",
    "        print('Input to Encoder')\n",
    "        print(weights_inp_encoder)\n",
    "        print('Encoder to Output')\n",
    "        print(weights_encoder_out)\n",
    "        #'''\n",
    "        weights_encoder_out = weights_encoder_out - learning_rate * delta_weights_encoder_out\n",
    "        weights_inp_encoder = weights_inp_encoder - learning_rate * delta_weights_inp_encoder\n",
    "       # '''\n",
    "        print('Weights Before Update')\n",
    "        print('Input to Encoder')\n",
    "        print(weights_inp_encoder)\n",
    "        print('Encoder to Output')\n",
    "        print(weights_encoder_out)\n",
    "        print('')\n",
    "        print('------------------------')\n",
    "        print('')\n",
    "        #'''\n",
    "        \n",
    "        k=k+1\n",
    "    #Once function converges, the final hidden layer is saved as \"encoder\" \n",
    "    \n",
    "    #Initialize the dataframe to keep track of the loss over the iterations(to be used in graph)\n",
    "    loss_df = pd.DataFrame(columns=['Loss'])\n",
    "    loss_df = pd.DataFrame(columns=['Loss'])\n",
    "    \n",
    "    \n",
    "    #Performs one hot encoding of class column for classification tasks in order to be used for the target predicted comparison\n",
    "    if task_type == 'Classification':\n",
    "        output_size=len(classes) \n",
    "        target_encoded=np.zeros((len(target),output_size))  \n",
    "        for i in range(0,len(target)):\n",
    "            class_index = classes.index(target[i])\n",
    "            target_encoded[i][class_index]=1\n",
    "        target=target_encoded\n",
    "        \n",
    "    #Sets the output size to 1 for regression tasks\n",
    "    if task_type == 'Regression':\n",
    "        output_size = 1\n",
    "    \n",
    "    #Initializes weights for the input to 1st hidden layer, 1st hidden layer to 2nd hidden layer,\n",
    "    #and the second hidden layer to the output layer\n",
    "    if train==True:\n",
    "        weights_inp_h1= np.random.uniform(-0.01,0.01,(num_features, nodes_sizes[0]))\n",
    "        weights_h1_h2 = np.random.uniform(-0.01,0.01,(nodes_sizes[0], nodes_sizes[1]))\n",
    "        weights_h2_out= np.random.uniform(-0.01,0.01,(nodes_sizes[1],output_size))\n",
    "        \n",
    "    #Use trained weights as weights during testing  \n",
    "    if test==True:\n",
    "        weights_inp_h1= trained_weights[2]\n",
    "        weights_h1_h2 = trained_weights[3]\n",
    "        weights_h2_out= trained_weights[4]\n",
    "    \n",
    "\n",
    "    #Begin loop to make predictions and update weights\n",
    "    print(encoder)\n",
    "    l=0\n",
    "    while l<iterations:    \n",
    "\n",
    "        #Same process as 2 layer feedforward network function except that the first hidden layer is set as the \n",
    "        #traind autoencoder from the first iteration\n",
    "        \n",
    "        #FEDEDFORWARD\n",
    "        hidden_1 = encoder\n",
    "\n",
    "        print('1st Hidden layer in autoencoder')\n",
    "        print(encoder)\n",
    "        print('')\n",
    "        \n",
    "        z_h2 = np.dot(hidden_1, weights_h1_h2)\n",
    "        hidden_2 = sigmoid(z_h2)\n",
    "        \n",
    "        z_output = np.dot(hidden_2, weights_h2_out)\n",
    "        prediction = sigmoid(z_output)\n",
    "        target=target.reshape(prediction.shape)\n",
    "     \n",
    "    \n",
    "         #When testing, return the predicted and final arrays as the output after 1 iteration\n",
    "        if test == True:\n",
    "            return [prediction,target]\n",
    "        \n",
    "        loss_df=loss_df.append({\"Loss\":Loss(target,prediction,task_type)},ignore_index=True)\n",
    "        \n",
    "        #BACKPROPAGATION\n",
    "        weights_out_h2 = (prediction - target) * prediction * (1 - prediction)\n",
    "        weights_h2_h1 = (np.dot(weights_out_h2, weights_h2_out.T)) * hidden_2 * (1 - hidden_2)\n",
    "        weights_h1_inp = (np.dot(weights_h2_h1, weights_h1_h2.T)) * hidden_1 * (1 - hidden_1)\n",
    "\n",
    "\n",
    "        # Updating the weights\n",
    "        delta_weights_h2_out = np.dot(hidden_2.T,weights_out_h2)/sample_size\n",
    "        delta_weights_h1_h2 = np.dot(hidden_1.T, weights_h2_h1) / sample_size\n",
    "        delta_weights_inp_h1 = np.dot(features.T, weights_h1_inp) / sample_size\n",
    "        \n",
    "        l=l+1\n",
    "        \n",
    "    #Doesn't produce loss graph when tuning hidden layers\n",
    "    if tuning == False:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "        encoder_loss_df.Loss.plot(title='Encoder Training Loss',ax=axes[0])\n",
    "        loss_df.Loss.plot(title='Final Model Loss',ax=axes[1])\n",
    "        \n",
    "    #returns the final weight trained values from the encoder training as well as the final model training\n",
    "    return [weights_inp_encoder,weights_encoder_out, weights_inp_h1,weights_h1_h2, weights_h2_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9c4d6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58fdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(task_type, true_values, predicted_values):\n",
    "    '''\n",
    "    Evaluates the data based on task type\n",
    "    \n",
    "    @param task_type: specifies whether the task is 'Classification' or 'Regression'\n",
    "    @param true_values: actual values of the data\n",
    "    @param predicted_values: values predicted by the model\n",
    "    \n",
    "    '''\n",
    "    #Calculates mean square error for regression data\n",
    "    if task_type == 'Regression':\n",
    "        mean_squared_error = np.square(np.subtract(true_values,predicted_values)).mean()\n",
    "        return mean_squared_error\n",
    "    \n",
    "    #Calculates classification accuracy and error rate for classification data\n",
    "    elif task_type == 'Classification':\n",
    "        classification_accuracy = Accuracy(predicted_values,true_values)\n",
    "        error_rate = 1-classification_accuracy\n",
    "        return classification_accuracy, error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a0338f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunExperiment(dataset, task_type, model_type='linear', k = None, class_label = None):\n",
    "    '''\n",
    "    Runs the experiments using the processed data, performs cross validation to get testing and training data,\n",
    "    standardizes the data, feeds into the algorithm, and uses the output to evaluate the accuracy\n",
    "    \n",
    "    @param dataset: pre-processed dataframe\n",
    "    @param validation_type: specifies whether the task is 'k-fold' or 'kx2 '\n",
    "    @param task_type: specifies whether the task is 'Classification' or 'Regression'\n",
    "    @param num_folds: number of folds--only needed when validation type is k-fold\n",
    "    @param k: k value to be used for cross validation-only needed when validation type is kx2\n",
    "    @param class_label: name of class column\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    classification = False\n",
    "    if task_type == 'classification' or task_type == 'Classification':\n",
    "        classification = True\n",
    "     \n",
    "    avg_accuracy = 0\n",
    "    avg_error = 0\n",
    "    avg_MSE = 0\n",
    "    \n",
    "    if model_type == 'FF' or model_type == 'Autoencoder':\n",
    "        nodes_sizes = TuneHidden(model_type,task_type,dataset, class_label,len(dataset.columns)-1)\n",
    "\n",
    "    #Performs k x 2 cross validation with parameter data\n",
    "    validation_output = k_x_2_cross_validation(dataset, k, classification, class_label)\n",
    "    for k in validation_output:\n",
    "        print('K = ' + str(k))\n",
    "        for num in validation_output[k]:\n",
    "            print('  Experiment ' + str(num))\n",
    "\n",
    "            #Gets test and training data for each k\n",
    "            test_data = validation_output[k][num]['test']\n",
    "            train_data = validation_output[k][num]['train']\n",
    "            '''\n",
    "            print('Data Before Standardization: ')\n",
    "            print('train')\n",
    "            print(train_data)\n",
    "            print('test')\n",
    "            print(test_data)\n",
    "            print('')\n",
    "            '''\n",
    "            #Standardizes test and training data\n",
    "            standardized_data = Standardize(train_data,test_data)\n",
    "            train_data = standardized_data[0]\n",
    "            test_data = standardized_data[1]\n",
    "\n",
    "            '''\n",
    "            print('Data After Standardization: ')\n",
    "            print('train')\n",
    "            print(train_data)\n",
    "            print('test')\n",
    "            print(test_data)\n",
    "            '''\n",
    "\n",
    "            print('   Test Fold Size: ' + str(len(test_data)))\n",
    "            print('   Train Fold Size: ' + str(len(train_data)))\n",
    "\n",
    "            #Gets the prediction value from the models\n",
    "            if model_type == 'linear':\n",
    "                if task_type == 'Classification':\n",
    "                    trained_weights = LogisticRegression(train_data,class_label,train=True)\n",
    "                    model_output = LogisticRegression(test_data,class_label,test=True,trained_weights=trained_weights)\n",
    "                if task_type == 'Regression':\n",
    "                    trained_weights = LinearRegressionNetwork(train_data,class_label,train=True)\n",
    "                    model_output = LinearRegressionNetwork(test_data,class_label,test=True,trained_weights=trained_weights)\n",
    "            if model_type == 'FF':\n",
    "                trained_weights=FeedForwardNN_2H(train_data,class_label,nodes_sizes,task_type,train=True)\n",
    "                model_output=FeedForwardNN_2H(test_data,class_label,nodes_sizes,task_type,test=True,trained_weights=trained_weights)\n",
    "            if model_type == 'Autoencoder':\n",
    "                trained_weights=Autoencoder(train_data,class_label,nodes_sizes, task_type,train=True)\n",
    "                model_output= Autoencoder(test_data,class_label,nodes_sizes, task_type,test=True,trained_weights=trained_weights)\n",
    "           \n",
    "            predicted_values = model_output[0]\n",
    "            true_values = model_output[1]\n",
    "                \n",
    "           \n",
    "            #Uses the true values from the test set, and predicted values from the model to evaluate accuracy/error\n",
    "            evaluation = Evaluate(task_type,true_values,predicted_values)\n",
    "            #Gets corresponding evaluation metric depending on task type and prints to console\n",
    "            if task_type == 'Classification':\n",
    "                accuracy = evaluation[0]\n",
    "                error = evaluation[1]\n",
    "                avg_accuracy = avg_accuracy + accuracy\n",
    "                avg_error = avg_error + error\n",
    "                print('    Classification Accuracy: '+ str(accuracy))\n",
    "                print('    Error: ' + str(error))\n",
    "            elif task_type == 'Regression':\n",
    "                MSE = evaluation\n",
    "                avg_MSE = avg_MSE + MSE\n",
    "                print('    Mean Squared Error: ' + str(MSE))\n",
    "            print('')\n",
    "\n",
    "    #Gets the average of the evaluation metric over all folds and prints it\n",
    "    if task_type == 'Classification':\n",
    "            print('Average Classification Accuracy: '+ str(avg_accuracy/(2*k)))\n",
    "            print('Average Error: ' + str(avg_error/(2*k)))\n",
    "    elif task_type == 'Regression':\n",
    "        print('Average Mean Squared Error: ' + str(avg_MSE/(2*k)))\n",
    "    print('')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
